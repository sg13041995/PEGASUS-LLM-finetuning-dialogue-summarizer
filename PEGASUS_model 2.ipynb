{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7l4vGtOsCPY"
      },
      "source": [
        "### Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwv4bHLYjc0u",
        "outputId": "4047bdb1-469a-4887-cdc0-43423718641c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Apr 20 05:58:24 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Checking the GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmphx6KzoVm8"
      },
      "source": [
        "We are using T4 GPU (Tesla 4 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn-Pbfsqi16q"
      },
      "outputs": [],
      "source": [
        "# Installing some packages\n",
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It-XXFBzpPid"
      },
      "source": [
        "- `-q` flag for quiet mode (no output)\n",
        "\n",
        "- **transformers:** This is a popular library for working with state-of-the-art natural language processing (NLP) models, especially transformers like BERT and GPT-2. The `[sentencepiece]` part specifies an optional extra dependency for the SentencePiece tokenizer, which is often used by transformer models.\n",
        "\n",
        "- **datasets:** This library provides a collection of ready-to-use NLP datasets and tools for loading, processing, and evaluating them.\n",
        "\n",
        "- **sacrebleu:** This package calculates the SacreBLEU score, a metric used to evaluate the quality of machine translation by comparing a generated translation to a reference translation.\n",
        "\n",
        "- **rouge_score:** This package calculates ROUGE scores, another set of metrics for evaluating machine translation quality.\n",
        "\n",
        "- **py7zr:** This library handles 7z archives, allowing you to work with datasets compressed in this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPut8GjRl7gt",
        "outputId": "809156ce-a05e-466e-b073-b72e55df3424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed accelerate-0.29.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Found existing installation: transformers 4.38.2\n",
            "Uninstalling transformers-4.38.2:\n",
            "  Successfully uninstalled transformers-4.38.2\n",
            "Found existing installation: accelerate 0.29.3\n",
            "Uninstalling accelerate-0.29.3:\n",
            "  Successfully uninstalled accelerate-0.29.3\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Using cached accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: tokenizers, transformers, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "Successfully installed accelerate-0.29.3 tokenizers-0.19.1 transformers-4.40.0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Found existing installation: transformers 4.40.0\n",
            "Uninstalling transformers-4.40.0:\n",
            "  Successfully uninstalled transformers-4.40.0\n",
            "Found existing installation: accelerate 0.29.3\n",
            "Uninstalling accelerate-0.29.3:\n",
            "  Successfully uninstalled accelerate-0.29.3\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: transformers, accelerate\n",
            "Successfully installed accelerate-0.29.3 transformers-4.40.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zd8EAunrM6u"
      },
      "source": [
        "- **!pip install --upgrade accelerate:**\n",
        "    - This line upgrades the accelerate package using pip.\n",
        "    - The accelerate package helps with training and running machine learning models on multiple GPUs (Graphics Processing Units).\n",
        "\n",
        "- **!pip uninstall -y transformers accelerate:**\n",
        "    - This line uninstalls both transformers and accelerate packages.\n",
        "    - The -y flag tells pip to proceed without confirmation prompts.\n",
        "\n",
        "- **!pip install transformers accelerate:**\n",
        "    - This line reinstalls both transformers and accelerate packages.\n",
        "  \n",
        "- **There are two possible reasons for this seemingly redundant approach:**\n",
        "    - **Dependency Conflict:** It's possible there was a conflict between the versions of transformers and accelerate you had installed previously. Uninstalling both and reinstalling them together ensures they are compatible versions.\n",
        "    - **Specific Version Requirement:** There might be a specific version requirement for transformers or accelerate for your project. This approach ensures you get the exact versions you need.\n",
        "\n",
        "Without more context about your project setup, it's difficult to say for sure. But in essence, this code snippet manages the installation of transformers and accelerate packages, likely to address a dependency issue or ensure specific version requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGzYEpessLf2"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM3ORFYNjKRH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset, load_from_disk, load_metric\n",
        "\n",
        "from transformers import (AutoModelForSeq2SeqLM,\n",
        "                          AutoTokenizer,\n",
        "                          DataCollatorForSeq2Seq,\n",
        "                          pipeline,\n",
        "                          TrainingArguments,\n",
        "                          Trainer)\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ruaZk-Vtm3m"
      },
      "source": [
        "**`torch`**: This library is a popular framework for deep learning and scientific computing. It's likely used for tensor operations and model computations in this context.\n",
        "\n",
        "**From datasets library:**\n",
        "- `load_dataset`: This function allows you to load various NLP datasets from the Datasets Hub.\n",
        "- `load_from_disk`: This function can be used to load datasets that were previously downloaded and saved to disk.\n",
        "- `load_metric`: This function helps you load evaluation metrics for NLP tasks, allowing you to assess model performance.\n",
        "\n",
        "**From transformers library:**\n",
        "\n",
        "- `AutoModelForSeq2SeqLM`: This allows you to automatically load any pre-trained sequence-to-sequence language model (Seq2SeqLM) supported by the library.\n",
        "- `AutoTokenizer`: This provides functionality to automatically load the tokenizer associated with the chosen Seq2SeqLM model.\n",
        "- `DataCollatorForSeq2Seq`: This class helps prepare batches of data for training or evaluation in a Seq2SeqLM model. It ensures proper formatting and handling of different data elements like dialogue, summary, and attention masks.\n",
        "- `pipeline`: This function allows you to create pipelines for various NLP tasks offered by the transformers library. This could be useful for deploying the model for real-time summarization tasks.\n",
        "- `TrainingArguments`: This class allows you to define and configure various training parameters for your model. These parameters can include things like:\n",
        "    - Number of training epochs\n",
        "    - Batch size\n",
        "    - Learning rate\n",
        "    - Optimizer selection\n",
        "    - Gradient accumulation steps\n",
        "    - Whether to perform evaluation during training\n",
        "    - Output directory for saving checkpoints and logs\n",
        "- `Trainer`: This class provides a high-level interface for training and evaluating a model using the configurations defined in TrainingArguments. It handles the training loop, logging, checkpointing, and evaluation, simplifying the training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_2Y1Agn7pj7P",
        "outputId": "8bef45af-7918-422a-bae5-3247e7bfc996"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setting device to GPU if available else CPU (with PyTorch)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OtUkvuStW9a"
      },
      "source": [
        "**`device = \"cuda\" if torch.cuda.is_available() else \"cpu\"`:** This line sets the device on which computations will be performed.\n",
        "\n",
        "- `torch.cuda.is_available()`:\n",
        "    - This checks if a CUDA-enabled NVIDIA GPU is available on your system.\n",
        "    - If a GPU is available, \"cuda\" is assigned to the device variable, indicating calculations will be done on the GPU for faster processing (assuming your model and data fit in GPU memory).\n",
        "    - If no GPU is found, \"cpu\" is assigned to device, indicating computations will be done on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxvLZ4Zkjf82"
      },
      "outputs": [],
      "source": [
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ysOggrGwEoF"
      },
      "source": [
        "1. **Model Checkpoint Path:**\n",
        "\n",
        "`model_ckpt = \"google/pegasus-cnn_dailymail\":` This line defines a variable named model_ckpt and assigns it a string value, \"google/pegasus-cnn_dailymail\". This string represents the identifier (checkpoint) for a pre-trained Pegasus model on the CNN/Daily Mail dataset, likely trained (fine tuned) for summarization tasks.\n",
        "\n",
        "2. **Loading Tokenizer:**\n",
        "\n",
        "`tokenizer = AutoTokenizer.from_pretrained(model_ckpt)`: This line uses the AutoTokenizer function from transformers to load the tokenizer associated with the model checkpoint specified in model_ckpt. The tokenizer is an essential part of the pipeline as it converts text into numerical representations (tokens) that the model can understand and process.\n",
        "\n",
        "3. **Loading and Moving Model:**\n",
        "\n",
        "`model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)`:\n",
        "\n",
        "This line performs two actions:\n",
        "- **Loading the Model:** It uses the AutoModelForSeq2SeqLM function from transformers to load the actual Pegasus model architecture based on the checkpoint specified in model_ckpt.\n",
        "- **Moving to Device:** It then uses the .to(device) method to move the loaded model to the device specified by the device variable (defined earlier). Recall that device is either \"cuda\" (GPU) or \"cpu\" depending on availability. This ensures computations happen on the chosen device for efficiency.\n",
        "\n",
        "In essence, this code sets up the necessary components (tokenizer and model) for text processing tasks, likely leveraging the pre-trained Pegasus model for summarization based on the chosen checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVubMBaaf0bL"
      },
      "source": [
        "### Data Import and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vg_V-Kpjmo1",
        "outputId": "1a5935a3-e0f4-4800-a859-2b4913a74f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-20 06:00:22--  https://github.com/sg13041995/Datasets/raw/main/textSummarizer_samsun.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sg13041995/Datasets/main/textSummarizer_samsun.zip [following]\n",
            "--2024-04-20 06:00:23--  https://raw.githubusercontent.com/sg13041995/Datasets/main/textSummarizer_samsun.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7903594 (7.5M) [application/zip]\n",
            "Saving to: ‘textSummarizer_samsun.zip’\n",
            "\n",
            "textSummarizer_sams 100%[===================>]   7.54M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-20 06:00:23 (130 MB/s) - ‘textSummarizer_samsun.zip’ saved [7903594/7903594]\n",
            "\n",
            "Archive:  textSummarizer_samsun.zip\n",
            "  inflating: samsum-test.csv         \n",
            "  inflating: samsum-train.csv        \n",
            "  inflating: samsum-validation.csv   \n",
            "   creating: samsum_dataset/\n",
            " extracting: samsum_dataset/dataset_dict.json  \n",
            "   creating: samsum_dataset/test/\n",
            "  inflating: samsum_dataset/test/data-00000-of-00001.arrow  \n",
            "  inflating: samsum_dataset/test/dataset_info.json  \n",
            "  inflating: samsum_dataset/test/state.json  \n",
            "   creating: samsum_dataset/train/\n",
            "  inflating: samsum_dataset/train/data-00000-of-00001.arrow  \n",
            "  inflating: samsum_dataset/train/dataset_info.json  \n",
            "  inflating: samsum_dataset/train/state.json  \n",
            "   creating: samsum_dataset/validation/\n",
            "  inflating: samsum_dataset/validation/data-00000-of-00001.arrow  \n",
            "  inflating: samsum_dataset/validation/dataset_info.json  \n",
            "  inflating: samsum_dataset/validation/state.json  \n"
          ]
        }
      ],
      "source": [
        "# Download and unzip the dataset for fine tuning\n",
        "!wget https://github.com/sg13041995/Datasets/raw/main/textSummarizer_samsun.zip\n",
        "!unzip textSummarizer_samsun.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pO1I7rPj-nc",
        "outputId": "49d14b37-9564-4925-c37a-829d4eb6513c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 14732\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 819\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the dataset from disk and explore\n",
        "dataset_samsum = load_from_disk('samsum_dataset')\n",
        "dataset_samsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2WU6zKiTOhH",
        "outputId": "96843f7f-322a-4d4c-f8a8-05717d042f1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "datasets.dataset_dict.DatasetDict"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(dataset_samsum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSp332V7SMrt",
        "outputId": "f624b1e9-66f7-486b-91d8-935516e4548f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'test', 'validation'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMRD3zTnSRru",
        "outputId": "01bd6003-8ef0-43e7-cfb4-4ce38a08fcea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_values([Dataset({\n",
              "    features: ['id', 'dialogue', 'summary'],\n",
              "    num_rows: 14732\n",
              "}), Dataset({\n",
              "    features: ['id', 'dialogue', 'summary'],\n",
              "    num_rows: 819\n",
              "}), Dataset({\n",
              "    features: ['id', 'dialogue', 'summary'],\n",
              "    num_rows: 818\n",
              "})])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgZNKXdpR34q",
        "outputId": "96bf3d20-5dce-46e3-cf0a-2bcb574c5f1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'dialogue', 'summary'],\n",
              "    num_rows: 14732\n",
              "})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "1KMHeevhTcWi",
        "outputId": "adde37b8-57ef-4a5c-b283-1e6fca32d14d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>datasets.arrow_dataset.Dataset</b><br/>def __init__(arrow_table: Table, info: Optional[DatasetInfo]=None, split: Optional[NamedSplit]=None, indices_table: Optional[Table]=None, fingerprint: Optional[str]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py</a>A Dataset backed by an Arrow table.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 668);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "datasets.arrow_dataset.Dataset"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(dataset_samsum[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNQmj1HzTz-2",
        "outputId": "c2dfb198-9530-4b78-9472-39bfcde66684"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id', 'dialogue', 'summary']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum['train'].column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz9SMzrSS-oY",
        "outputId": "b8075a2c-d194-4d57-8e52-61801da1486b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '13818513',\n",
              " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
              " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54qdc7B7UGdL",
        "outputId": "be625185-54ec-4bb8-faaf-610dcc4b118d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(dataset_samsum[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LPpreAVQUCcO",
        "outputId": "4563d0aa-3767-4834-d89f-c1d101d262e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'13818513'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum[\"train\"][0][\"id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6EgNcrKUFaO",
        "outputId": "8d947de6-3b24-4d50-dc9f-fb7129bfcec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amanda: I baked  cookies. Do you want some?\r\n",
            "Jerry: Sure!\r\n",
            "Amanda: I'll bring you tomorrow :-)\n"
          ]
        }
      ],
      "source": [
        "print(dataset_samsum[\"train\"][0][\"dialogue\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmzqTs_WUSvl",
        "outputId": "52e2fef7-b46c-443f-c728-4ff0b5c57795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amanda baked cookies and will bring Jerry some tomorrow.\n"
          ]
        }
      ],
      "source": [
        "print(dataset_samsum[\"train\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhjk-HUhgDc6"
      },
      "source": [
        "### Data Preprocessing and Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjqrjnHOkibG"
      },
      "outputs": [],
      "source": [
        "# Fine tuning data preparation\n",
        "def convert_examples_to_features(example_batch):\n",
        "    # Tokenizing the dialogue\n",
        "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
        "\n",
        "    # Tokenizing the summary considering them as target\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHYXdQ_A1XYL"
      },
      "source": [
        "**Function Purpose:**\n",
        "\n",
        "This function takes a batch of examples (likely containing dialogue and corresponding summaries) and converts them into a format suitable for the transformers library's Seq2SeqLM models.\n",
        "\n",
        "**Argument:**\n",
        "`example_batch` is a dictionary-like object containing the data for a batch of examples.\n",
        "\n",
        "**Steps:**\n",
        "1. Encode Dialogue: `input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )`\n",
        "\n",
        "This line uses the loaded tokenizer (tokenizer) to convert the dialogue text in `example_batch['dialogue']` into numerical representations. `max_length = 1024` specifies the maximum allowed length for the encoded dialogue (sequence of tokens). Longer sequences will be truncated. `truncation = True` indicates that if the dialogue is longer than max_length, it will be shortened (truncated) to fit the limit.\n",
        "\n",
        "2. Encode Summary (with special target handling):\n",
        "\n",
        "`with tokenizer.as_target_tokenizer():` This line enters a context where the tokenizer treats the input text as a \"target\" sequence. This might be necessary because the tokenizer might handle target text differently than input text (e.g., adding special tokens for the beginning or end of the summary).\n",
        "\n",
        "`target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )`: This line is similar to the dialogue encoding but uses the tokenizer in \"target mode\" and applies it to the summary text in `example_batch['summary']`.\n",
        "\n",
        "3. Creating the Output Dictionary:\n",
        "- `'input_ids'`: This key stores the encoded dialogue sequence (`input_encodings['input_ids']`).\n",
        "- `'attention_mask'`: This key stores the attention mask (`input_encodings['attention_mask']`). The attention mask is used by the model to focus on relevant parts of the input sequence.\n",
        "- `'labels'`: This key stores the encoded summary sequence (`target_encodings['input_ids']`). It's called \"labels\" here, possibly because the model is being trained to predict the summary given the dialogue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYPE33LTVq5t"
      },
      "outputs": [],
      "source": [
        "# Testing the function on the first sample from train dataset\n",
        "tokenized_example = convert_examples_to_features(dataset_samsum[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by9yBcq4WNUO",
        "outputId": "9638ea08-e647-4037-b788-cc2253279970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[12195, 151, 125, 7091, 3659, 107, 842, 119, 245, 181, 152, 10508, 151, 7435, 147, 12195, 151, 125, 131, 267, 650, 119, 3469, 29344, 1]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_example[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9slvT0mHWg5D",
        "outputId": "581aca61-c4e1-454e-fa16-93391493e52e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_example[\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzdhiBEfWlq2",
        "outputId": "23127f64-b160-407e-e731-4192d71ecb32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[12195, 7091, 3659, 111, 138, 650, 10508, 181, 3469, 107, 1]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_example[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abi8Y2XvgAnY",
        "outputId": "020daf66-dd5a-4102-fd8f-1aa36e8ab156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded Dialogue: Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)\n",
            "Decoded Summary: Amanda baked cookies and will bring Jerry some tomorrow.\n"
          ]
        }
      ],
      "source": [
        "# Decoding the tokenized sample without including the special tokens\n",
        "\n",
        "decoded_dialogue = tokenizer.decode(tokenized_example[\"input_ids\"], skip_special_tokens=True)\n",
        "decoded_summary = tokenizer.decode(tokenized_example[\"labels\"], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Decoded Dialogue: {decoded_dialogue}\")\n",
        "print(f\"Decoded Summary: {decoded_summary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNffSTzRhHV9",
        "outputId": "ddc46ed2-f606-4adb-dc3d-8253514b3411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded Dialogue: Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)</s>\n",
            "Decoded Summary: Amanda baked cookies and will bring Jerry some tomorrow.</s>\n"
          ]
        }
      ],
      "source": [
        "# Decoding the tokenized sample including the special tokens\n",
        "\n",
        "decoded_dialogue = tokenizer.decode(tokenized_example[\"input_ids\"], skip_special_tokens=False)\n",
        "decoded_summary = tokenizer.decode(tokenized_example[\"labels\"], skip_special_tokens=False)\n",
        "\n",
        "print(f\"Decoded Dialogue: {decoded_dialogue}\")\n",
        "print(f\"Decoded Summary: {decoded_summary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoVgpDo_hXHR",
        "outputId": "1858625d-2153-4668-a891-169384b466dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoded Attention: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n"
          ]
        }
      ],
      "source": [
        "# Decoding the attention mask\n",
        "\n",
        "decoded_attention = tokenizer.decode(tokenized_example[\"attention_mask\"], skip_special_tokens=False)\n",
        "\n",
        "print(f\"Decoded Attention: {decoded_attention}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A7Y6vrkklMx",
        "outputId": "5e9ded25-b32a-488a-90ab-dc956a03f0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "11\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "# We can observe that the length of the attention mask is same as the number of input tokens\n",
        "# There was no padding and so the attention mask is all 1s\n",
        "\n",
        "print(len(tokenized_example[\"input_ids\"]))\n",
        "print(len(tokenized_example[\"labels\"]))\n",
        "print(len(tokenized_example[\"attention_mask\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "b8fe8ff5cdcc451297711b62bcb434f9",
            "5f342b63151643d59a6f9b70e31157fb",
            "b392f0bf143c4c04a5849514e715c546",
            "714562e0e8514885a40b69ec819c4e5d",
            "afbf9106d3c740cfaaaf65862690e167",
            "a869d2072b284d0c8c3fd03b6dcf5ad4",
            "69d01239167d46568c3d535f959c17ee",
            "32d02b49c86043f4bd0da2b6bd850c26",
            "c1bf47757f9c4d48b418b03ea3bddb8b",
            "c6aabfe204de478a9447880017e30960",
            "c861aa5ca8d64d52b9d74a58488916f1",
            "34d385f63f174317b73cf59ea95569b7",
            "6ffa80f10eab4b129ec5b6d40ccc8779",
            "1d56be8dd4e54b37bd6294a85e59e08c",
            "3e26acb3554a4c03b9062f6d098ddb85",
            "bc4d963cf1904f35bf97f1fe4cb3bce9",
            "3556a61b2ed54d988437a4ebd90c50fa",
            "6cf4668c56414cf486f9c8d9857da617",
            "018f44cb99df4209be1d48eb3cb258b7",
            "ad97a8dd3a8a475e9cd4fd6f0c223642",
            "4f11e946afff416ab926640bc1337d7c",
            "4a2275e1d0a749b0a69ae66422977c72",
            "6cb1da2b44ca49399ae551fe8316c6a5",
            "8ed28c4c8e8e49b398df2249d8d64909",
            "2f904b3c72de424caea66b41b9067c87",
            "5f7d535195ce4a9b9aa8ad96abccbb83",
            "5a63c978ca2e4498b695afd7e5972309",
            "a8c4764f589b41bc8f73eea0865112ba",
            "e16101dd22fa4dd2973c4df66545cec3",
            "588aa7de3c854ee7b673c4aeafdfbb24",
            "636f27e987044b8d84081857afcc8a87",
            "034027930fb14253b65e2e1079601751",
            "9fb5f33ff3e445dbb3c3599ab3a5c4df"
          ]
        },
        "id": "ovCPjEQOks2d",
        "outputId": "9738b8f9-5b75-4a15-b173-ef9aeb129c84"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8fe8ff5cdcc451297711b62bcb434f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34d385f63f174317b73cf59ea95569b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cb1da2b44ca49399ae551fe8316c6a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Applying the convert_examples_to_features function on the dataset\n",
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC1AbgA-kvCm",
        "outputId": "ca60f4e5-3512-4051-ee3c-6e823080410e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 14732\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_samsum_pt[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWZboBc5k2Ul"
      },
      "outputs": [],
      "source": [
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxatistNPFcb"
      },
      "source": [
        "This line creates an instance of the DataCollatorForSeq2Seq class from the transformers library. This class is specifically designed to handle data preparation for training and evaluation of sequence-to-sequence models (Seq2SeqLM) like the Pegasus model you loaded earlier.\n",
        "\n",
        "Here's a breakdown of how it's used:\n",
        "\n",
        "- DataCollatorForSeq2Seq: This is the class name used to create the data collator object.\n",
        "- tokenizer: This argument specifies the tokenizer you loaded previously (likely tokenizer) which is used to understand the structure and format the text data.\n",
        "- model=model_pegasus: This argument specifies the Seq2SeqLM model you loaded previously (likely model_pegasus). While not always required, providing the model can allow the data collator to perform optimizations specific to that model's requirements.\n",
        "\n",
        "In essence, this line creates a helper object (seq2seq_data_collator) that will be responsible for batching and preparing your data (dialogue and summaries) in a format suitable for training or evaluating your Pegasus model for summarization tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETrjK2FZlDY2"
      },
      "outputs": [],
      "source": [
        "# Defining the training configuration\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum',\n",
        "    num_train_epochs=4,\n",
        "    warmup_steps=200,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=50,\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpF6jbkwSbW8"
      },
      "source": [
        "`output_dir='pegasus-samsum'`: This argument specifies the directory where the trained model, checkpoints, and logging files will be saved. Here, it's set to \"pegasus-samsum\", suggesting the model is being trained for summarization tasks.\n",
        "\n",
        "`num_train_epochs=4`: This argument sets the number of times the entire training dataset will be passed through the model for training. Here, it's set to 1, indicating a single training epoch.\n",
        "\n",
        "`warmup_steps=200`: This argument specifies the number of steps during which the learning rate will be gradually increased from zero to its final value. This helps to stabilize the training process at the beginning. Here, the warmup will last for the first 200 training steps.\n",
        "\n",
        "`per_device_train_batch_size=1`: This argument defines the number of training examples included in each batch processed by the model on a single device (GPU or CPU). Here, a batch size of 1 is used, which means each training step will process a single dialogue-summary pair.\n",
        "\n",
        "`per_device_eval_batch_size=1`: This argument defines the number of examples included in each batch processed by the model during evaluation on a single device. Similar to training batch size, here it's set to 1, indicating a single example per evaluation step.\n",
        "\n",
        "`weight_decay=0.01`: This argument controls a regularization technique called weight decay that helps prevent overfitting. Here, a weight decay of 0.01 is applied.\n",
        "\n",
        "`logging_steps=10`: This argument specifies the frequency at which training metrics (loss, accuracy, etc.) are logged and printed to the console. Here, logs will be printed every 10 training steps.\n",
        "\n",
        "`evaluation_strategy='steps'`: This argument defines the strategy for performing model evaluation during training. Here, \"steps\" is chosen, indicating evaluation will occur at regular intervals based on the number of training steps.\n",
        "\n",
        "`eval_steps=50`: This argument defines the frequency of evaluation steps when using the \"steps\" evaluation strategy. Here, the model will be evaluated every 50 training steps.\n",
        "\n",
        "`save_steps=1e6`: This argument specifies the frequency at which the model checkpoint is saved during training. Here, the model will be saved every 1 million training steps (1e6).\n",
        "\n",
        "`gradient_accumulation_steps=16`: This argument allows accumulating gradients from multiple training steps before performing a parameter update. This technique can improve training speed and memory usage with large models. Here, gradients from 16 training steps will be accumulated before updating the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHrye9salGGW"
      },
      "outputs": [],
      "source": [
        "# Initializing the trainer object\n",
        "trainer = Trainer(model=model_pegasus,\n",
        "                  args=trainer_args,\n",
        "                  tokenizer=tokenizer,\n",
        "                  data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"test\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSOCL6AZI-0W"
      },
      "source": [
        "`model=model_pegasus`: This argument specifies the PEGASUS model instance that you want to fine-tune. It's assumed that you've already loaded the model using transformers.PegasusForConditionalGeneration.from_pretrained().\n",
        "\n",
        "`args=trainer_args`: This argument provides a dictionary containing training hyperparameters that control the training process. Common parameters include learning rate, number of training epochs, gradient accumulation steps, and early stopping criteria.\n",
        "\n",
        "`tokenizer=tokenizer`: This argument specifies the tokenizer that will be used to prepare your text data for the model. For PEGASUS summarization, you'll likely use PegasusTokenizer.from_pretrained(). The tokenizer handles tasks like vocabulary handling, tokenization, and padding.\n",
        "\n",
        "`data_collator=seq2seq_data_collator`: This argument is a function that collates individual data samples (text and summaries) into mini-batches suitable for training the model. It's particularly important for handling variable-length sequences in summarization. The Transformers library often provides pre-built data collators for common tasks like summarization.\n",
        "\n",
        "`train_dataset=dataset_samsum_pt[\"test\"]`: This argument specifies the dataset that will be used for training. Here, dataset_samsum_pt is assumed to be a dictionary containing dataset splits. You're using the \"test\" split, which might not be ideal for training (usually the \"train\" split is used). Make sure you're using the correct training split for your dataset.\n",
        "\n",
        "`eval_dataset=dataset_samsum_pt[\"validation\"]`: This argument specifies the dataset that will be used for evaluation during training. The model will be periodically evaluated on this split to monitor its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECRjIbf5gSx3"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "DwpPL9fMl1xU",
        "outputId": "13a28d77-6223-4fb1-a13a-e98cb5ca38f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [204/204 12:29, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.622600</td>\n",
              "      <td>2.072233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.991100</td>\n",
              "      <td>1.747815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.852700</td>\n",
              "      <td>1.630636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.626900</td>\n",
              "      <td>1.580894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=204, training_loss=2.138618700644549, metrics={'train_runtime': 753.9962, 'train_samples_per_second': 4.345, 'train_steps_per_second': 0.271, 'total_flos': 1252252679675904.0, 'train_loss': 2.138618700644549, 'epoch': 3.9853479853479854})"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model with 4 epochs based on the previous observation of overfitting after epoch 4\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okK6wOP6JUwI"
      },
      "source": [
        "- We observed overfitting after 4 epochs so we tried to stop the training at epoch 4\n",
        "- But the model is not really working well with a training till 4 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysjKvNHlgVSb"
      },
      "source": [
        "### Model Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJyDIKWZmoUj",
        "outputId": "522f1205-6db2-430c-a68e-871237e329b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w26e-0yemoRt",
        "outputId": "ac0dd698-d376-411f-a7c7-3c01e462062a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/spiece.model',\n",
              " 'tokenizer/added_tokens.json',\n",
              " 'tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(\"tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtYNSBuDTJXX"
      },
      "source": [
        "### Model Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVP4cS-7mLeH"
      },
      "outputs": [],
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiuXLptsUKAA"
      },
      "source": [
        "This code defines a function named generate_batch_sized_chunks that splits a list of elements into smaller batches. Here's a breakdown of how it works:\n",
        "\n",
        "**Function Purpose:**\n",
        "\n",
        "This function takes two arguments:\n",
        "- list_of_elements: This is a list containing the elements you want to split into batches.\n",
        "\n",
        "- batch_size: This is an integer specifying the desired size of each batch.\n",
        "The function iterates through the list and yields (returns) sublists of the specified batch_size.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "- Looping Through the List:\n",
        "for i in range(0, len(list_of_elements), batch_size):: This loop iterates over a range of indexes starting from 0, up to but not including the length of the list (len(list_of_elements)), and stepping by the batch_size.\n",
        "\n",
        "- Slicing and Yielding Batches:\n",
        "yield list_of_elements[i : i + batch_size]: Inside the loop, this line uses slicing to extract a sublist from the original list. The slice starts at index i (current loop position) and goes up to (but not including) i + batch_size. This ensures the sublist has the desired size.\n",
        "The yield keyword is used to return this sublist (batch) from the function without stopping the loop. The next iteration of the loop will create and yield the next batch of elements.\n",
        "\n",
        "Overall, this function provides a convenient way to iterate over a large list in smaller, manageable chunks. This can be useful for various tasks, such as training machine learning models in batches or processing large datasets piece by piece."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNS3kTb1TcK7"
      },
      "outputs": [],
      "source": [
        "def calculate_metric_on_test_ds(dataset,\n",
        "                                metric,\n",
        "                                model,\n",
        "                                tokenizer,\n",
        "                                batch_size=16,\n",
        "                                device=device,\n",
        "                                column_text=\"article\",\n",
        "                                column_summary=\"highlights\"):\n",
        "\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
        "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "        # Finally, we decode the generated texts, replace the  token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg3PhEsTWi8E"
      },
      "source": [
        "This code defines a function calculate_metric_on_test_ds that likely evaluates a model's performance on a test dataset using a specific metric (ROUGE score in this case) for a summarization task. Here's a breakdown of the function:\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "This function takes several arguments:\n",
        "\n",
        "- dataset: This is the test dataset containing examples (likely dialogue and corresponding summaries).\n",
        "\n",
        "- metric: This is a metric object (likely a ROUGE metric) used to evaluate the quality of generated summaries.\n",
        "\n",
        "- model: This is the pre-trained summarization model (likely Pegasus in this case).\n",
        "\n",
        "- tokenizer: This is the tokenizer associated with the model used for text processing.\n",
        "\n",
        "- batch_size: This is an optional argument specifying the number of examples to process in each batch (default 16).\n",
        "\n",
        "- device: This is an optional argument specifying the device to use for computations (CPU or GPU, likely set earlier).\n",
        "\n",
        "- column_text: This is an optional argument specifying the column name in the dataset containing the text to be summarized (default \"article\").\n",
        "\n",
        "- column_summary: This is an optional argument specifying the column name in the dataset containing the reference summaries (default \"highlights\").\n",
        "\n",
        "The function iterates through the test dataset in batches, generates summaries for each dialogue using the model, and then uses the metric object to compare the generated summaries with the reference summaries in the dataset. Finally, it returns the calculated metric score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8BgNHAfXIm6"
      },
      "source": [
        "**Steps:**\n",
        "\n",
        "1. Splitting Data into Batches:\n",
        "\n",
        "article_batches = ... target_batches = ...: These lines use the generate_batch_sized_chunks function (defined earlier) to split the text data (dataset[column_text]) and reference summaries (dataset[column_summary]) from the test dataset into batches of the specified batch_size.\n",
        "\n",
        "2. Looping Through Batches:\n",
        "\n",
        "for article_batch, target_batch in tqdm(...):: This loop iterates over the corresponding batches of text data and reference summaries. The tqdm progress bar shows the progress of the loop.\n",
        "\n",
        "3. Encoding Text Data:\n",
        "\n",
        "inputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\"): This line uses the tokenizer to encode the text data in article_batch for the model. It applies:\n",
        "max_length=1024: Maximum allowed length for encoded text.\n",
        "truncation=True: Truncates longer text to fit the limit.\n",
        "padding=\"max_length\": Pads shorter text to match the longest sequence.\n",
        "return_tensors=\"pt\": Converts the encoded data to PyTorch tensors for efficient model computations (if device is GPU).\n",
        "\n",
        "4. Generating Summaries:\n",
        "\n",
        "summaries = model.generate(...): This line calls the generate method of the model (likely Pegasus for summarization). It generates summaries for the encoded text in inputs based on the model's training. Here:\n",
        "input_ids: Encoded dialogue text (from inputs).\n",
        "attention_mask: Attention mask for the encoded text (from inputs).\n",
        "length_penalty=0.8: This parameter discourages generating very long summaries (configurable).\n",
        "num_beams=8: This parameter controls the beam search strategy for generating summaries (configurable).\n",
        "max_length=128: This parameter specifies the maximum allowed length for the generated summaries.\n",
        "\n",
        "5. Decoding Generated Summaries:\n",
        "\n",
        "decoded_summaries = [tokenizer.decode(s, ...)]: This line decodes the generated summaries from tokens back to human-readable text using the tokenizer. It applies:\n",
        "skip_special_tokens=True: Ignores special tokens added by the tokenizer during encoding.\n",
        "clean_up_tokenization_spaces: Cleans up any extra spaces introduced during tokenization.\n",
        "\n",
        "6. Replacing Empty Decoded Summaries:\n",
        "\n",
        "decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]: This line replaces any empty decoded summaries (which might occur due to generation issues) with a space to avoid errors in the metric calculation.\n",
        "\n",
        "7. Adding Summaries and References to Metric:\n",
        "\n",
        "metric.add_batch(predictions=..., references=...): This line adds the generated summaries (decoded_summaries) as predictions and the reference summaries (target_batch) from the dataset to the metric object (likely ROUGE metric) for evaluation.\n",
        "\n",
        "8. Calculating and Returning Metric Score:\n",
        "\n",
        "score = metric.compute(): This line calls the compute method of the metric object to calculate the final ROUGE score based on the accumulated predictions and references added earlier.\n",
        "return score: The function returns the calculated ROUGE score (score) as the output.\n",
        "\n",
        "Overall, this function provides a framework for evaluating a summarization model's performance on a test dataset using the ROUGE metric. It iterates through the dataset in batches, generates summaries for the text data, compares them with reference summaries, and calculates the overall ROUGE score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "be83f5762a40449c9a0153ed3f7e6a16",
            "800382d099d84931b2299c98f5700ff9",
            "25158ea9144f4c01be6a71ccef197bce",
            "3770574b965045628d083b7a74ef5299",
            "fd8f7f22813f466596910b65921ea987",
            "cdca8ae90d104a0dbd90492211ecae00",
            "23f6c18837334ab58edd3d298882fb75",
            "232812b0292e4d438e30f902bf536723",
            "50b7e46996304605a2862724f1338475",
            "a9ac0aca24554de7a1a0800eda0b2aaa",
            "6baaf67d32604eb283a2936346918429"
          ]
        },
        "id": "mxHIt7sGmYrN",
        "outputId": "a422e33d-f71b-42d3-e86d-d6e1529c024b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-46-5a43aadd1b0e>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric('rouge')\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be83f5762a40449c9a0153ed3f7e6a16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_metric = load_metric('rouge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqsyh3pscyCx"
      },
      "source": [
        "rouge_names: This list contains the names of specific ROUGE variants you want to calculate:\n",
        "\n",
        "- \"rouge1\": Refers to ROUGE-1 which considers the overlap of unigrams (single words) between the generated summary and the reference summaries.\n",
        "\n",
        "- \"rouge2\": Refers to ROUGE-2 which considers the overlap of bigrams (sequences of two words) between the generated summary and the reference summaries.\n",
        "\n",
        "- \"rougeL\": Refers to ROUGE-L which considers the longest common subsequence (LCS) of words between the generated summary and the reference summaries.\n",
        "\n",
        "- \"rougeLsum\": This is likely a custom name for ROUGE-L Recall, focusing on the recall aspect of the LCS-based evaluation. (Standard ROUGE-L considers both precision and recall).\n",
        "\n",
        "rouge_metric: This line uses the load_metric function from the datasets library to load the ROUGE metric with the specified configurations. Here, it likely loads the ROUGE metric while keeping the configurations flexible for future adjustments.\n",
        "\n",
        "In essence, this code prepares to use the ROUGE metric for evaluation, focusing on specific ROUGE variants for unigram, bigram overlap, and longest common subsequences. The custom \"rougeLsum\" possibly suggests an interest in the recall aspect of ROUGE-L."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "DRiJW_JbmYom",
        "outputId": "8d42c311-5d6d-42b2-e1db-588626462a57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 410/410 [13:00<00:00,  1.90s/it]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.01834327948083566,\n        \"max\": 0.01834327948083566,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01834327948083566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0002849603290117121,\n        \"max\": 0.0002849603290117121,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0002849603290117121\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.018281035886189657,\n        \"max\": 0.018281035886189657,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.018281035886189657\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.01828852188293873,\n        \"max\": 0.01828852188293873,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01828852188293873\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c62462ef-c4d8-42bd-a27f-85fbd49eafd3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pegasus</th>\n",
              "      <td>0.018343</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.018281</td>\n",
              "      <td>0.018289</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c62462ef-c4d8-42bd-a27f-85fbd49eafd3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c62462ef-c4d8-42bd-a27f-85fbd49eafd3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c62462ef-c4d8-42bd-a27f-85fbd49eafd3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           rouge1    rouge2    rougeL  rougeLsum\n",
              "pegasus  0.018343  0.000285  0.018281   0.018289"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'],\n",
        "    rouge_metric,\n",
        "    trainer.model,\n",
        "    tokenizer,\n",
        "    batch_size = 2,\n",
        "    column_text = 'dialogue',\n",
        "    column_summary= 'summary'\n",
        ")\n",
        "\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM-ZSwitNjLJ"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoXp_HobNldA",
        "outputId": "dc0b2bc1-ea15-4ed3-c7d9-19ddcbc0eadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Dialogue:\n",
            "\n",
            " Hannah: Hey, do you have Betty's number?\n",
            "Amanda: Lemme check\n",
            "Hannah: <file_gif>\n",
            "Amanda: Sorry, can't find it.\n",
            "Amanda: Ask Larry\n",
            "Amanda: He called her last time we were at the park together\n",
            "Hannah: I don't know him well\n",
            "Hannah: <file_gif>\n",
            "Amanda: Don't be shy, he's very nice\n",
            "Hannah: If you say so..\n",
            "Hannah: I'd rather you texted him\n",
            "Amanda: Just text him 🙂\n",
            "Hannah: Urgh.. Alright\n",
            "Hannah: Bye\n",
            "Amanda: Bye bye\n",
            "\n",
            "Summary:\n",
            " Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
          ]
        }
      ],
      "source": [
        "# Looking at specific example from the test dataset\n",
        "\n",
        "test_example_number = 0\n",
        "sample_text = dataset_samsum[\"test\"][test_example_number][\"dialogue\"]\n",
        "reference = dataset_samsum[\"test\"][test_example_number][\"summary\"]\n",
        "\n",
        "print(\"Input Dialogue:\\n\\n\", sample_text)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Summary:\\n\", reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnDJOnjlOAMk",
        "outputId": "02037066-bd1a-4541-b038-2102fe589fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "459\n",
            "54\n"
          ]
        }
      ],
      "source": [
        "# Looking at the length of input and target(summary) sequences for the specific example\n",
        "\n",
        "print(len(sample_text))\n",
        "print(len(reference))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6EBGcGcOp-R",
        "outputId": "54134609-b041-4325-e501-b95c93705a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: 407\n",
            "Summary 83\n",
            "Ratio 0.20393120393120392\n",
            "==================================================\n",
            "Input: 459\n",
            "Summary 54\n",
            "Ratio 0.11764705882352941\n",
            "==================================================\n",
            "Input: 592\n",
            "Summary 150\n",
            "Ratio 0.2533783783783784\n",
            "==================================================\n",
            "Input: 461\n",
            "Summary 50\n",
            "Ratio 0.10845986984815618\n",
            "==================================================\n",
            "Input: 1101\n",
            "Summary 221\n",
            "Ratio 0.20072661217075385\n",
            "==================================================\n",
            "Input: 1559\n",
            "Summary 300\n",
            "Ratio 0.19243104554201412\n",
            "==================================================\n",
            "Input: 1055\n",
            "Summary 190\n",
            "Ratio 0.18009478672985782\n",
            "==================================================\n",
            "Input: 439\n",
            "Summary 60\n",
            "Ratio 0.1366742596810934\n",
            "==================================================\n",
            "Input: 479\n",
            "Summary 138\n",
            "Ratio 0.2881002087682672\n",
            "==================================================\n",
            "Input: 427\n",
            "Summary 96\n",
            "Ratio 0.22482435597189696\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Looking at the length of input and target(summary) sequences and (summary/input) ratio for some examples\n",
        "\n",
        "for i in range(10):\n",
        "  test_example_number = i\n",
        "  sample_text = dataset_samsum[\"test\"][test_example_number][\"dialogue\"]\n",
        "  reference = dataset_samsum[\"test\"][test_example_number][\"summary\"]\n",
        "\n",
        "  print(\"Input:\", len(sample_text))\n",
        "  print(\"Summary\", len(reference))\n",
        "  print(\"Ratio\", len(reference)/len(sample_text))\n",
        "  print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrL2Bm3bRaMD"
      },
      "outputs": [],
      "source": [
        "# Decided the multipliers based on the above observed ratios\n",
        "min_length_multiplier = 0.10\n",
        "max_length_multiplier = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KnXN_KWSefP",
        "outputId": "c190587c-8c16-4b27-8dde-e0dd644b887f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40.7\n",
            "101.75\n",
            "\n",
            "83\n"
          ]
        }
      ],
      "source": [
        "# Checking the calculated min_length and max_length as per the multipliers\n",
        "\n",
        "test_example_number = 0\n",
        "sample_text = dataset_samsum[\"test\"][test_example_number][\"dialogue\"]\n",
        "reference = dataset_samsum[\"test\"][test_example_number][\"summary\"]\n",
        "\n",
        "print(len(sample_text)*min_length_multiplier)\n",
        "print(len(sample_text)*max_length_multiplier)\n",
        "\n",
        "print()\n",
        "\n",
        "print(len(reference))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5zcWzn4UbTQ"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyt2HbnXVaGd"
      },
      "outputs": [],
      "source": [
        "# Summarization parameter settings\n",
        "\n",
        "test_example_number = 0\n",
        "sample_text = dataset_samsum[\"test\"][test_example_number][\"dialogue\"]\n",
        "reference = dataset_samsum[\"test\"][test_example_number][\"summary\"]\n",
        "\n",
        "min_length = int(len(sample_text)*min_length_multiplier)\n",
        "max_length = int(len(sample_text)*max_length_multiplier)\n",
        "\n",
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"min_length\": min_length, \"max_length\": max_length}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7wVFucnSrdN",
        "outputId": "7433dfe8-163b-4027-d0a7-1cedf9286668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dialogue:\n",
            "Hannah: Hey, do you have Betty's number?\n",
            "Amanda: Lemme check\n",
            "Hannah: <file_gif>\n",
            "Amanda: Sorry, can't find it.\n",
            "Amanda: Ask Larry\n",
            "Amanda: He called her last time we were at the park together\n",
            "Hannah: I don't know him well\n",
            "Hannah: <file_gif>\n",
            "Amanda: Don't be shy, he's very nice\n",
            "Hannah: If you say so..\n",
            "Hannah: I'd rather you texted him\n",
            "Amanda: Just text him 🙂\n",
            "Hannah: Urgh.. Alright\n",
            "Hannah: Bye\n",
            "Amanda: Bye bye\n",
            "\n",
            "Reference Summary:\n",
            "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
            "\n",
            "Model Summary:\n",
            "Betty's number is Larry's. He called her last time they were at the park together. He's very nice. Hannah would rather she text him instead of finding Betty's number.\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\", tokenizer=tokenizer)\n",
        "\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnD7ad0ZN4J"
      },
      "source": [
        "The summary looks distorted. Not really good."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "w7l4vGtOsCPY",
        "TGzYEpessLf2",
        "gVubMBaaf0bL"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "018f44cb99df4209be1d48eb3cb258b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "034027930fb14253b65e2e1079601751": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d56be8dd4e54b37bd6294a85e59e08c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018f44cb99df4209be1d48eb3cb258b7",
            "max": 819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad97a8dd3a8a475e9cd4fd6f0c223642",
            "value": 819
          }
        },
        "232812b0292e4d438e30f902bf536723": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f6c18837334ab58edd3d298882fb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25158ea9144f4c01be6a71ccef197bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_232812b0292e4d438e30f902bf536723",
            "max": 2169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50b7e46996304605a2862724f1338475",
            "value": 2169
          }
        },
        "2f904b3c72de424caea66b41b9067c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588aa7de3c854ee7b673c4aeafdfbb24",
            "max": 818,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_636f27e987044b8d84081857afcc8a87",
            "value": 818
          }
        },
        "32d02b49c86043f4bd0da2b6bd850c26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34d385f63f174317b73cf59ea95569b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ffa80f10eab4b129ec5b6d40ccc8779",
              "IPY_MODEL_1d56be8dd4e54b37bd6294a85e59e08c",
              "IPY_MODEL_3e26acb3554a4c03b9062f6d098ddb85"
            ],
            "layout": "IPY_MODEL_bc4d963cf1904f35bf97f1fe4cb3bce9"
          }
        },
        "3556a61b2ed54d988437a4ebd90c50fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3770574b965045628d083b7a74ef5299": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9ac0aca24554de7a1a0800eda0b2aaa",
            "placeholder": "​",
            "style": "IPY_MODEL_6baaf67d32604eb283a2936346918429",
            "value": " 5.65k/? [00:00&lt;00:00, 389kB/s]"
          }
        },
        "3e26acb3554a4c03b9062f6d098ddb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f11e946afff416ab926640bc1337d7c",
            "placeholder": "​",
            "style": "IPY_MODEL_4a2275e1d0a749b0a69ae66422977c72",
            "value": " 819/819 [00:00&lt;00:00, 1911.29 examples/s]"
          }
        },
        "4a2275e1d0a749b0a69ae66422977c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f11e946afff416ab926640bc1337d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b7e46996304605a2862724f1338475": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "588aa7de3c854ee7b673c4aeafdfbb24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a63c978ca2e4498b695afd7e5972309": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f342b63151643d59a6f9b70e31157fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a869d2072b284d0c8c3fd03b6dcf5ad4",
            "placeholder": "​",
            "style": "IPY_MODEL_69d01239167d46568c3d535f959c17ee",
            "value": "Map: 100%"
          }
        },
        "5f7d535195ce4a9b9aa8ad96abccbb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_034027930fb14253b65e2e1079601751",
            "placeholder": "​",
            "style": "IPY_MODEL_9fb5f33ff3e445dbb3c3599ab3a5c4df",
            "value": " 818/818 [00:00&lt;00:00, 1900.46 examples/s]"
          }
        },
        "636f27e987044b8d84081857afcc8a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69d01239167d46568c3d535f959c17ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6baaf67d32604eb283a2936346918429": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cb1da2b44ca49399ae551fe8316c6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ed28c4c8e8e49b398df2249d8d64909",
              "IPY_MODEL_2f904b3c72de424caea66b41b9067c87",
              "IPY_MODEL_5f7d535195ce4a9b9aa8ad96abccbb83"
            ],
            "layout": "IPY_MODEL_5a63c978ca2e4498b695afd7e5972309"
          }
        },
        "6cf4668c56414cf486f9c8d9857da617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ffa80f10eab4b129ec5b6d40ccc8779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3556a61b2ed54d988437a4ebd90c50fa",
            "placeholder": "​",
            "style": "IPY_MODEL_6cf4668c56414cf486f9c8d9857da617",
            "value": "Map: 100%"
          }
        },
        "714562e0e8514885a40b69ec819c4e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6aabfe204de478a9447880017e30960",
            "placeholder": "​",
            "style": "IPY_MODEL_c861aa5ca8d64d52b9d74a58488916f1",
            "value": " 14732/14732 [00:08&lt;00:00, 2054.84 examples/s]"
          }
        },
        "800382d099d84931b2299c98f5700ff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdca8ae90d104a0dbd90492211ecae00",
            "placeholder": "​",
            "style": "IPY_MODEL_23f6c18837334ab58edd3d298882fb75",
            "value": "Downloading builder script: "
          }
        },
        "8ed28c4c8e8e49b398df2249d8d64909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8c4764f589b41bc8f73eea0865112ba",
            "placeholder": "​",
            "style": "IPY_MODEL_e16101dd22fa4dd2973c4df66545cec3",
            "value": "Map: 100%"
          }
        },
        "9fb5f33ff3e445dbb3c3599ab3a5c4df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a869d2072b284d0c8c3fd03b6dcf5ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8c4764f589b41bc8f73eea0865112ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9ac0aca24554de7a1a0800eda0b2aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad97a8dd3a8a475e9cd4fd6f0c223642": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afbf9106d3c740cfaaaf65862690e167": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b392f0bf143c4c04a5849514e715c546": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32d02b49c86043f4bd0da2b6bd850c26",
            "max": 14732,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1bf47757f9c4d48b418b03ea3bddb8b",
            "value": 14732
          }
        },
        "b8fe8ff5cdcc451297711b62bcb434f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f342b63151643d59a6f9b70e31157fb",
              "IPY_MODEL_b392f0bf143c4c04a5849514e715c546",
              "IPY_MODEL_714562e0e8514885a40b69ec819c4e5d"
            ],
            "layout": "IPY_MODEL_afbf9106d3c740cfaaaf65862690e167"
          }
        },
        "bc4d963cf1904f35bf97f1fe4cb3bce9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be83f5762a40449c9a0153ed3f7e6a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_800382d099d84931b2299c98f5700ff9",
              "IPY_MODEL_25158ea9144f4c01be6a71ccef197bce",
              "IPY_MODEL_3770574b965045628d083b7a74ef5299"
            ],
            "layout": "IPY_MODEL_fd8f7f22813f466596910b65921ea987"
          }
        },
        "c1bf47757f9c4d48b418b03ea3bddb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6aabfe204de478a9447880017e30960": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c861aa5ca8d64d52b9d74a58488916f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdca8ae90d104a0dbd90492211ecae00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16101dd22fa4dd2973c4df66545cec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd8f7f22813f466596910b65921ea987": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
